# Blind AI - Real-Time Scene Narrator for the Visually Impaired

**Blind AI** is a real-time scene understanding assistant designed to help visually impaired users understand their environment. It uses a fusion of advanced computer vision and language models to provide clear, grounded, and natural scene descriptions.

---

## ðŸ§  Core Technologies

- **YOLOv11s** â€” Real-time object detection
- **BLIP** â€” Image captioning
- **LLaMA-2 7B via Ollama** â€” Local language model for reasoning and narration
- **pyttsx3** â€” Offline text-to-speech engine with customizable voices
- **OpenCV** â€” Video stream processing and display

---

## ðŸš€ Features

- Describes surroundings using live webcam feed
- Object-aware and spatially grounded narration
- Polite, natural language descriptions tailored for blind users
- Customizable voice (e.g. British English Female)
- Optional BLIP captioning for enriched understanding
- NO hallucinations, emojis, roleplay, or self-references

---

## ðŸ“¦ Folder Structure

```
blind_ai/
â”œâ”€â”€ main.py                    # Main orchestrator script
â”œâ”€â”€ detectors/
â”‚   â””â”€â”€ yolo_detector.py       # YOLOv11 object detection
â”œâ”€â”€ captioning/
â”‚   â””â”€â”€ blip_captioner.py      # BLIP caption generation
â”œâ”€â”€ language/
â”‚   â””â”€â”€ llama_client.py        # Local LLaMA prompt + response
â”œâ”€â”€ speech/
â”‚   â””â”€â”€ speaker.py             # Text-to-speech using pyttsx3
```

---

## ðŸ› ï¸ Setup Instructions

### 1. Install Requirements

```bash
pip install -r requirements.txt
```

You will need:
- Python 3.11+
- `ultralytics` (for YOLOv11)
- `transformers` (for BLIP)
- `torch`
- `pyttsx3`
- `opencv-python`
- `requests`

> âš ï¸ Make sure Ollama is running locally with LLaMA-2 loaded:
```bash
ollama run llama2
```

---

### 2. Run the Assistant

```bash
python main.py
```

Press:
- `[SPACE]` â€” to capture and describe the current frame
- `[Q]` â€” to quit

---

## ðŸ§  How It Works

1. Captures a frame from your webcam
2. Detects objects using YOLOv11s and assigns spatial positions (left, center, right)
3. Optionally generates a caption using BLIP
4. Feeds object list + (optional) caption into LLaMA-2
5. LLaMA responds with a **grounded**, **polite**, and **helpful** narration
6. pyttsx3 converts that text into spoken output

---

## ðŸ—£ï¸ Custom Voice Setup (macOS)

Use a British English female voice (like `Kate`, `Serena`, or `Samantha`):

1. Open **System Settings > Accessibility > Spoken Content > System Voice > Customize**
2. Download the voice you want
3. In `speaker.py`, set:
```python
self.engine.setProperty("voice", "com.apple.voice.enhanced.en-GB.Kate")
```

---

## ðŸ§ª Prompt Behavior (Strict Style)

- No greetings like "Sure, let me describe..."
- No questions at the end
- No roleplay, gestures, or feelings
- First-person or third-person options
- Describes only objects **actually detected**

---

## âœ… Example Output

> "To your left, I see a chair. In front of you, there is a person standing. A bottle is on the right, slightly farther away."

---

## ðŸ”„ Want to Customize It?
- Swap `YOLOv11s` with another YOLO variant (e.g. `yolo11n.pt`)
- Turn off BLIP completely
- Add prompt logging for hallucination auditing

---

## ðŸ“© Contributions & Feedback

This is an open project. Feel free to fork, experiment, and improve!

If you run into issues or want to suggest upgrades, reach out. Letâ€™s build assistive tech that actually helps. ðŸ’¡


